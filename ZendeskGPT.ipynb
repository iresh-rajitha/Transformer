{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ### TPU Initialization\n",
        "\n"
      ],
      "metadata": {
        "id": "MBlYuCi9FbnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a TPU is available and initialize it\n",
        "import tensorflow as tf\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "    # print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('Connected to TPU')\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('Running on CPU/GPU')\n",
        "\n",
        "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ArGd5nrE-Vs",
        "outputId": "9459cfa5-359b-412d-ab5e-b9368e23b9b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.48.206.90:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.48.206.90:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to TPU\n",
            "REPLICAS: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HyperParameters"
      ],
      "metadata": {
        "id": "wdhjhG8UCxB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000 # should replace\n",
        "d_model = 128\n",
        "nhead = 8\n",
        "num_encoder_layers = 6\n",
        "num_classes = 56  # Change this according to your problem\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "Az6G0OsUCpKE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dummy data"
      ],
      "metadata": {
        "id": "OdcrBzz8C2op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_sequence_length = 50  # Adjust according to your actual data\n",
        "# num_samples = 1000"
      ],
      "metadata": {
        "id": "kyFOEGQtDTe9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# # Dummy data for demonstration purposes\n",
        "\n",
        "\n",
        "# # Generate random team descriptions and labels\n",
        "# team_descriptions = [\" \".join(np.random.choice(vocab_size, size=np.random.randint(5, 20), replace=True).astype(str)) for _ in range(num_samples)]\n",
        "# labels = np.random.randint(num_classes, size=num_samples)\n",
        "\n",
        "# # Tokenize team descriptions and pad sequences\n",
        "# tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "# tokenizer.fit_on_texts(team_descriptions)\n",
        "# sequences = tokenizer.texts_to_sequences(team_descriptions)\n",
        "# padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# # Create a dummy train_dataset\n"
      ],
      "metadata": {
        "id": "UnaWY4VLCrTV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/data.pkl', 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "\n",
        "padded_sequences = loaded_data['questions']\n",
        "labels = loaded_data['answers']\n",
        "vocab_size = loaded_data['VOCAB_SIZE']\n",
        "num_classes = loaded_data['total_teams']\n"
      ],
      "metadata": {
        "id": "ESVDCi9hT3Rt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((padded_sequences, labels)).shuffle(num_samples).batch(batch_size=32)"
      ],
      "metadata": {
        "id": "y83oG2fVT1-k"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0]"
      ],
      "metadata": {
        "id": "QDMYSq0fD_4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences[0]"
      ],
      "metadata": {
        "id": "1wUg12UbD_T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Model"
      ],
      "metadata": {
        "id": "p1Hx3MWRDXH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XLmNu3El-mfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac45ebc-7201-455e-ad1c-ca70e99f9066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, None, 128)   527488      ['embedding_7[0][0]',            \n",
            " eadAttention)                                                    'embedding_7[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, None, 128)    0           ['multi_head_attention_6[0][0]', \n",
            "                                                                  'embedding_8[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, None, 128)   256         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, None, 128)   527488      ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, None, 128)    0           ['multi_head_attention_7[0][0]', \n",
            "                                                                  'embedding_9[0][0]']            \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, None, 128)   256         ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_8 (MultiH  (None, None, 128)   527488      ['layer_normalization_7[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " embedding_10 (Embedding)       (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, None, 128)    0           ['multi_head_attention_8[0][0]', \n",
            "                                                                  'embedding_10[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, None, 128)   256         ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_9 (MultiH  (None, None, 128)   527488      ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " embedding_11 (Embedding)       (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, None, 128)    0           ['multi_head_attention_9[0][0]', \n",
            "                                                                  'embedding_11[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, None, 128)   256         ['add_9[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_10 (Multi  (None, None, 128)   527488      ['layer_normalization_9[0][0]',  \n",
            " HeadAttention)                                                   'layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " embedding_12 (Embedding)       (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, None, 128)    0           ['multi_head_attention_10[0][0]',\n",
            "                                                                  'embedding_12[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, None, 128)   256         ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_11 (Multi  (None, None, 128)   527488      ['layer_normalization_10[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " embedding_13 (Embedding)       (None, None, 128)    6015360     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, None, 128)    0           ['multi_head_attention_11[0][0]',\n",
            "                                                                  'embedding_13[0][0]']           \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, None, 128)   256         ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1 (Gl  (None, 128)         0           ['layer_normalization_11[0][0]'] \n",
            " obalAveragePooling1D)                                                                            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 56)           7224        ['global_average_pooling1d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 45,281,208\n",
            "Trainable params: 45,281,208\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, MultiHeadAttention, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_transformer_model(vocab_size, d_model, nhead, num_encoder_layers, num_classes):\n",
        "    inputs = Input(shape=(None,))\n",
        "    x = Embedding(vocab_size, d_model)(inputs)\n",
        "\n",
        "    for _ in range(num_encoder_layers):\n",
        "        # Self-attention layer (multi-head)\n",
        "        x = MultiHeadAttention(num_heads=nhead, key_dim=d_model)(x, x)\n",
        "        # Add and Norm\n",
        "        x = tf.keras.layers.Add()([x, Embedding(vocab_size, d_model)(inputs)])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "# Define your dataset and DataLoader here\n",
        "\n",
        "# Assuming you have a vocabulary size, embedding dimension, etc.\n",
        "\n",
        "\n",
        "# Create model\n",
        "with strategy.scope():\n",
        "  model = create_transformer_model(vocab_size, d_model, nhead, num_encoder_layers, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "model.summary()\n",
        "# Now you can use train_dataset in the training loop\n",
        "# Training loop\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     total_loss = 0\n",
        "#     total_accuracy = 0\n",
        "#     batches = 0\n",
        "\n",
        "#     for inputs, labels in train_dataset:\n",
        "#         loss, accuracy = model.train_on_batch(inputs, labels)\n",
        "#         total_loss += loss\n",
        "#         total_accuracy += accuracy\n",
        "#         batches += 1\n",
        "\n",
        "#     average_loss = total_loss / batches\n",
        "#     average_accuracy = total_accuracy / batches\n",
        "\n",
        "#     print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, epochs=num_epochs, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLN9Y2o-MK6U",
        "outputId": "dbc0aff0-eab2-43f3-8100-f5839ca92cd3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 208s 556ms/step - loss: 2.5137 - accuracy: 0.2120\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 153s 488ms/step - loss: 2.4461 - accuracy: 0.2078\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 153s 489ms/step - loss: 2.4408 - accuracy: 0.2031\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 153s 488ms/step - loss: 2.4407 - accuracy: 0.2090\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 153s 488ms/step - loss: 2.4350 - accuracy: 0.2200\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 153s 489ms/step - loss: 2.4343 - accuracy: 0.2137\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 153s 489ms/step - loss: 2.4329 - accuracy: 0.2085\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 153s 488ms/step - loss: 2.4300 - accuracy: 0.2175\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 153s 488ms/step - loss: 2.4329 - accuracy: 0.2171\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 153s 489ms/step - loss: 2.4303 - accuracy: 0.2163\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79acac9d02e0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a new team description in the form of a sequence\n",
        "new_team_description = \"your new team description here\"\n",
        "\n",
        "# Tokenize and pad the new team description\n",
        "new_sequence = tokenizer.texts_to_sequences([new_team_description])\n",
        "padded_new_sequence = pad_sequences(new_sequence, maxlen=max_sequence_length)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_new_sequence)\n",
        "\n",
        "# Print the predicted class probabilities\n",
        "print(\"Predicted Probabilities:\", predictions)\n",
        "\n",
        "# Get the predicted class (index with the maximum probability)\n",
        "predicted_class = np.argmax(predictions)\n",
        "print(\"Predicted Class:\", predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZbZwp0r8Vos",
        "outputId": "e0a37c48-8d52-43dd-a0d1-00d9e10516e9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Predicted Probabilities: [[2.8022507e-02 1.7055346e-01 1.2558098e-01 3.0308437e-02 8.2716621e-02\n",
            "  1.2518027e-03 2.2175602e-04 2.4074170e-01 1.7088924e-01 1.2620963e-02\n",
            "  1.3386293e-03 2.8831733e-03 2.3919649e-03 5.0834976e-03 7.6606398e-04\n",
            "  3.0035194e-04 4.8170374e-03 3.9225840e-03 1.1030734e-03 1.3615083e-04\n",
            "  1.2850510e-03 1.1460884e-03 3.1798388e-04 6.9174457e-05 9.8312208e-05\n",
            "  3.4435291e-03 1.1493506e-02 2.6982187e-04 1.9310595e-02 1.8061996e-02\n",
            "  7.5875776e-04 4.9647558e-03 5.4325154e-03 1.2273801e-02 8.0744579e-04\n",
            "  1.5066161e-04 1.5797639e-04 1.0800840e-04 3.9705189e-04 1.2368596e-03\n",
            "  1.0250230e-04 9.4714045e-04 7.3127019e-05 1.1730097e-02 4.2959619e-03\n",
            "  7.7021662e-03 9.2421327e-04 6.8289874e-04 6.4318633e-04 4.1043584e-04\n",
            "  7.6580465e-05 3.0787953e-03 1.5409822e-03 7.6849297e-05 2.0187182e-04\n",
            "  7.9308702e-05]]\n",
            "Predicted Class: 7\n"
          ]
        }
      ]
    }
  ]
}