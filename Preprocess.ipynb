{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:44:57.771908918Z",
     "start_time": "2023-11-01T07:44:57.577096294Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "# data_set_url = '/home/ireshr/PageroLabs/Full_dataset/secure_archive/dataset.csv'\n",
    "data_set_url = '/home/ireshr/PageroLabs/11-1/dataset_acrhive/processed-dataset.csv'\n",
    "b_data=pd.read_csv( data_set_url , lineterminator='\\n' , header = None)\n",
    "b_data=b_data.astype(str)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:44:57.772164054Z",
     "start_time": "2023-11-01T07:44:57.617919567Z"
    }
   },
   "id": "7aaa67722b31128c"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "subject_array = b_data[1][:10000]\n",
    "description_array = b_data[2][:10000]\n",
    "teams = b_data[3][:10000]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:44:57.809610664Z",
     "start_time": "2023-11-01T07:44:57.767854287Z"
    }
   },
   "id": "10dc3f829ad48a74"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "9999"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_array.size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:44:57.809859635Z",
     "start_time": "2023-11-01T07:44:57.809460826Z"
    }
   },
   "id": "d126fd31d40811b0"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    subject_array + description_array, target_vocab_size=2**18)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:45:08.304792926Z",
     "start_time": "2023-11-01T07:44:57.857342831Z"
    }
   },
   "id": "c1254b18579f39d5"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:45:08.307288215Z",
     "start_time": "2023-11-01T07:45:08.305867462Z"
    }
   },
   "id": "6208ff4fc1c30506"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "from datetime import  datetime\n",
    "\n",
    "now = datetime.now()\n",
    "tokenizer.save_to_file('./dictionary_'+ now.strftime(\"%m %d %Y, %H:%M:%S\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:45:08.353811446Z",
     "start_time": "2023-11-01T07:45:08.308053446Z"
    }
   },
   "id": "a1ccd2eaddf87d2"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "START_TOKEN, END_TOKEN , SUBJECT_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1] , [tokenizer.vocab_size + 2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:45:08.354169321Z",
     "start_time": "2023-11-01T07:45:08.353548526Z"
    }
   },
   "id": "d2fe9dffcc85795c"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[123], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m         inputs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n\u001B[0;32m----> 8\u001B[0m ticket_input \u001B[38;5;241m=\u001B[39m \u001B[43mconcat_subject_and_description\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubject_array\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdescription_array\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[123], line 4\u001B[0m, in \u001B[0;36mconcat_subject_and_description\u001B[0;34m(subjects, descriptions)\u001B[0m\n\u001B[1;32m      2\u001B[0m inputs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (subject, description) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(subjects, descriptions):\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mSUBJECT_TOKEN\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msubject\u001B[49m \u001B[38;5;241m+\u001B[39m SUBJECT_TOKEN \u001B[38;5;241m+\u001B[39m description\n\u001B[1;32m      5\u001B[0m     inputs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "def concat_subject_and_description(subjects, descriptions):\n",
    "    inputs = []\n",
    "    for (subject, description) in zip(subjects, descriptions):\n",
    "        input = SUBJECT_TOKEN + subject + SUBJECT_TOKEN + description\n",
    "        inputs.append(input)\n",
    "    return inputs\n",
    "\n",
    "ticket_input = concat_subject_and_description(subject_array, description_array)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:45:08.354573268Z",
     "start_time": "2023-11-01T07:45:08.353712037Z"
    }
   },
   "id": "e32651175844558b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def maximum_input_size(inputs):\n",
    "    max = len(inputs[0])\n",
    "    for element in inputs:\n",
    "        max = max if max > len(element) else len(element)\n",
    "    return max\n",
    "maximum_input_size(ticket_input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T07:45:08.400268552Z",
     "start_time": "2023-11-01T07:45:08.397609541Z"
    }
   },
   "id": "27fdd7be7e6168e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100 \n",
    "#  This is just a random value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T07:45:08.397747185Z"
    }
   },
   "id": "88c0a1bca517ea8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize, filter and pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "        # check tokenized sentence max length\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "questions, answers = tokenize_and_filter(ticket_input, teams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T07:45:08.397802700Z"
    }
   },
   "id": "fcf32318c558f637"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
