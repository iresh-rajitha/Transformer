{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pagero 3rd Line GPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd7554e3669af037"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import general libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87db4a4e8a66815f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import sys\n",
    "\n",
    "# !pip install tensorflow==2.11.0\n",
    "# !pip install tensorflow==2.14.0\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# !pip install tensorflow-datasets==4.1.0\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tensorflow version {}\".format(tf.__version__))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d08504b3a8a299c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize TPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab0e700ec2116d4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1009373c8836f288"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### HyperParameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f33e7690dd9ad33b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ATTEMPT = 1\n",
    "# Maximum sentence length, subject+ description word length\n",
    "MAX_LENGTH = 20 \n",
    "\n",
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 70000 # 0 = for All of data otherwise mention the size\n",
    "\n",
    "# Cut off value of words in the dictionary\n",
    "TRESHOLD_VALUE = 1\n",
    "\n",
    "# For tf.data.Dataset\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
    "BUFFER_SIZE = 20000 #Shuffle data in the dataset\n",
    "\n",
    "# For Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 16\n",
    "UNITS = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "EPOCHS = 4\n",
    "TRAINING_RATIO = 0.9"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b4ed6a91123e93c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f026b4a50c6ccfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "\n",
    "tickets = loaded_data['questions']\n",
    "teams = loaded_data['answers']\n",
    "VOCAB_SIZE = loaded_data['VOCAB_SIZE']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "557a5c7d34128778"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "training_len = math.floor(len(tickets)*TRAINING_RATIO)\n",
    "\n",
    "train_questions = tickets[:training_len]\n",
    "train_answers = teams[:training_len]\n",
    "\n",
    "test_questions = tickets[training_len:]\n",
    "test_answers = teams[training_len:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aad28c64f8a6d17e"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T09:41:31.562512895Z",
     "start_time": "2023-12-03T09:41:31.358620378Z"
    }
   },
   "id": "2ac8b8a19f03d567"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_dataset(data_questions, data_answers):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            'inputs': data_questions,\n",
    "        },\n",
    "        {\n",
    "            'outputs': data_answers\n",
    "        },\n",
    "    ))\n",
    "\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1784a993369ac80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_questions,train_answers)\n",
    "test_dataset = make_dataset(test_questions,test_answers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdcf8ba98e53981"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Vocab size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf724c3cd2d7d540"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feed to transformer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50d023cec043b1b7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiHeadAttention.call() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# initialize and compile model within strategy scope\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mscope():\n\u001B[0;32m---> 20\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mVOCAB_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNUM_LAYERS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43munits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUNITS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[43md_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mD_MODEL\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNUM_HEADS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDROPOUT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer, loss\u001B[38;5;241m=\u001B[39mloss_function, metrics\u001B[38;5;241m=\u001B[39m[accuracy])\n\u001B[1;32m     30\u001B[0m model\u001B[38;5;241m.\u001B[39msummary()\n",
      "File \u001B[0;32m~/Pagero/GPT/Transformer/transformer/Transformer.py:30\u001B[0m, in \u001B[0;36mtransformer\u001B[0;34m(vocab_size, num_layers, units, d_model, num_heads, dropout, name)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# mask the encoder outputs for the 2nd attention block\u001B[39;00m\n\u001B[1;32m     26\u001B[0m dec_padding_mask \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mLambda(\n\u001B[1;32m     27\u001B[0m     create_padding_mask, output_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m     28\u001B[0m     name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdec_padding_mask\u001B[39m\u001B[38;5;124m'\u001B[39m)(inputs)\n\u001B[0;32m---> 30\u001B[0m enc_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43munits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43md_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m(inputs\u001B[38;5;241m=\u001B[39m[inputs, enc_padding_mask])\n\u001B[1;32m     39\u001B[0m dec_outputs \u001B[38;5;241m=\u001B[39m decoder(\n\u001B[1;32m     40\u001B[0m     vocab_size\u001B[38;5;241m=\u001B[39mvocab_size,\n\u001B[1;32m     41\u001B[0m     num_layers\u001B[38;5;241m=\u001B[39mnum_layers,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     45\u001B[0m     dropout\u001B[38;5;241m=\u001B[39mdropout,\n\u001B[1;32m     46\u001B[0m )(inputs\u001B[38;5;241m=\u001B[39m[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n\u001B[1;32m     48\u001B[0m outputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(units\u001B[38;5;241m=\u001B[39mvocab_size, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)(dec_outputs)\n",
      "File \u001B[0;32m~/Pagero/GPT/Transformer/transformer/encoder/Encoder.py:24\u001B[0m, in \u001B[0;36mencoder\u001B[0;34m(vocab_size, num_layers, units, d_model, num_heads, dropout, name)\u001B[0m\n\u001B[1;32m     21\u001B[0m outputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDropout(rate\u001B[38;5;241m=\u001B[39mdropout)(embeddings)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_layers):\n\u001B[0;32m---> 24\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m        \u001B[49m\u001B[43munits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43munits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43md_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoder_layer_\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m([outputs, padding_mask])\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mModel(\n\u001B[1;32m     33\u001B[0m     inputs\u001B[38;5;241m=\u001B[39m[inputs, padding_mask], outputs\u001B[38;5;241m=\u001B[39moutputs, name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[0;32m~/Pagero/GPT/Transformer/transformer/encoder/Encoder.py:41\u001B[0m, in \u001B[0;36mencoder_layer\u001B[0;34m(units, d_model, num_heads, dropout, name)\u001B[0m\n\u001B[1;32m     38\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, d_model), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m padding_mask \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpadding_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 41\u001B[0m attention \u001B[38;5;241m=\u001B[39m \u001B[43mMultiHeadAttention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43md_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mquery\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkey\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvalue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_mask\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[43m\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m attention \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDropout(rate\u001B[38;5;241m=\u001B[39mdropout)(attention)\n\u001B[1;32m     49\u001B[0m add_attention \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39madd([inputs, attention])\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001B[0m, in \u001B[0;36m_call_unconverted\u001B[0;34m(f, args, kwargs, options, update_cache)\u001B[0m\n\u001B[1;32m    456\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__self__\u001B[39m\u001B[38;5;241m.\u001B[39mcall(args, kwargs)\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 459\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs)\n",
      "\u001B[0;31mTypeError\u001B[0m: MultiHeadAttention.call() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "from LossFunction import loss_function\n",
    "from transformer.Transformer import transformer\n",
    "from CustomScheduler import CustomSchedule\n",
    "\n",
    "# clear backend\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# initialize and compile model within strategy scope\n",
    "with strategy.scope():\n",
    "    model = transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T09:44:58.155142298Z",
     "start_time": "2023-12-03T09:44:57.958882703Z"
    }
   },
   "id": "7d094da7d76db9f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fc6eb698093a3e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "history = model.fit(train_dataset, validation_data= test_dataset, epochs=EPOCHS )\n",
    "filename = 'weights.h5'\n",
    "model.save_weights(filename)\n",
    "training_time = time() - start_time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed4e884f0a2e44af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35ded36b9b86bb23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y= 0.15\n",
    "r= .02\n",
    "params_print = [\"ACCURACY :\"+ str(history.history['accuracy'][-1]),\n",
    "                \"VAL_ACCURACY :\"+ str(history.history['val_accuracy'][-1]),\n",
    "                \"------ TRANSFORMER---------- :\",\n",
    "                \"UNITS :\"+ str(UNITS),\n",
    "                \"LAYERS :\"+ str(NUM_LAYERS),\n",
    "                \"DROPOUT :\"+ str(DROPOUT),\n",
    "                \"D_MODEL :\"+ str(D_MODEL),\n",
    "                \"NUM_HEADS :\"+ str(NUM_HEADS),\n",
    "                \"------ DATASET---------- :\",\n",
    "                \"BATCH_SIZE :\"+ str(BATCH_SIZE),\n",
    "                \"TRAINING_RATIO :\"+ str(TRAINING_RATIO),\n",
    "                \"BUFFER_SIZE :\"+ str(BUFFER_SIZE),\n",
    "                \"------ TRAINING---------- :\",\n",
    "                \"EPOCHS:\"+ str(EPOCHS),\n",
    "                \"TIME:\"+ str(format(training_time,\".3f\")),\n",
    "                \"TIME:\"+ str(training_time//60)+\" m\"+ str(training_time%60)+\" s\",\n",
    "                \"REPLICAS:\"+ str(strategy.num_replicas_in_sync)\n",
    "                ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d99e584f2851d9a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
