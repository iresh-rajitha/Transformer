{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Load Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "951e90ef9b7711fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'weights.h5'\n",
    "\n",
    "# For Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 16\n",
    "UNITS = 512\n",
    "DROPOUT = 0.3\n",
    "\n",
    "EPOCHS = 4\n",
    "TRAINING_RATIO = 0.9\n",
    "VOCAB_SIZE = 0 \n",
    "# All hyper param should ;load with a file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a7defde651ac34d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformer.Transformer import transformer\n",
    "\n",
    "new_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "new_model.load_weights(filename)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f93b2fab74bb49be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def evaluate(sentence):\n",
    "#     # sentence = preprocess_sentence(sentence)\n",
    "#     sentence = sinhala_pre.preprocess_sentence(sentence)\n",
    "# \n",
    "#     sentence = tf.expand_dims(\n",
    "#         START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "# \n",
    "#     output = tf.expand_dims(START_TOKEN, 0)\n",
    "# \n",
    "#     for i in range(MAX_LENGTH):\n",
    "#         predictions = model(inputs=[sentence, output], training=False)\n",
    "# \n",
    "#         # select the last word from the seq_len dimension\n",
    "#         predictions = predictions[:, -1:, :]\n",
    "#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "# \n",
    "#         # return the result if the predicted_id is equal to the end token\n",
    "#         if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "#             break\n",
    "# \n",
    "#         # concatenated the predicted_id to the output which is given to the decoder\n",
    "#         # as its input.\n",
    "#         output = tf.concat([output, predicted_id], axis=-1)\n",
    "# \n",
    "#     return tf.squeeze(output, axis=0)\n",
    "# \n",
    "# \n",
    "# def predict(sentence):\n",
    "#     # word_index = []\n",
    "#     prediction = evaluate(sentence).numpy()\n",
    "#     # print(prediction)\n",
    "#     # print(type(prediction))\n",
    "#     # predicted_sentence = ''\n",
    "#     predicted_sentence = tokenizer.decode(\n",
    "#         [i for i in prediction if i < tokenizer.vocab_size])\n",
    "# \n",
    "#     print('Input: {}'.format(sentence))\n",
    "#     print('Indexes: {}'.format(prediction))\n",
    "#     print('Output: {}'.format(predicted_sentence))\n",
    "# \n",
    "#     return predicted_sentence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8ed0a756dc5089"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
